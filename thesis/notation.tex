
\nomenclature{$\mathbb{P}(x)$}{Probability of event $x$}
\nomenclature{$s$}{state}
\nomenclature{$a$}{action}
\nomenclature{$r(s,a)$}{reward received after performing action $a$ in state $s$}
\nomenclature{$R(s,a,s')$}{reward received after performing action $a$ in state $s$ and landing in $s'$}
\nomenclature{$\mathcal{S}$}{state space}
\nomenclature{$\mathcal{A}$}{action space}
\nomenclature{$\Delta (\mathcal{X})$}{set of  probability distributions over space $\mathcal{X}$}
\nomenclature{$s_t$}{state at time $t$}
\nomenclature{$t$}{discrete time step $t$}
\nomenclature{$a_t$}{action at time $t$}
\nomenclature{$\Theta$}{family of policy parameters $\Theta \subseteq \mathbb{R}^d$}
\nomenclature{$\boldsymbol{\theta}$}{policy parameter vector $\boldsymbol{\theta} \in \mathbb{R}^d$}
\nomenclature{$\Omega$}{family of model parameters $\Omega \subseteq \mathbb{R}^{d'}$}
\nomenclature{$\boldsymbol{\omega}$}{model parameter vector $\boldsymbol{\omega} \in \mathbb{R}^{d'}$}
\nomenclature{$\pi$}{policy}
\nomenclature{$\Pi$}{policy space}
\nomenclature{$\mathcal{P}$}{configuration space, also referred to as model space}
\nomenclature{$P$}{model configuration, environment dynamics}
\nomenclature{$\pi_{\boldsymbol{\theta}}$}{policy parametrized by $\boldsymbol{\theta}$}
\nomenclature{$P_{\boldsymbol{\omega}}$}{model parametrized by $\boldsymbol{\omega}$}
\nomenclature{$x \sim \mathcal{D}(\cdot)$}{sample $x$ comes from the distribution $\mathcal{D}(\cdot)$}
\nomenclature{$\underset{x \sim \mathcal{D}}{\mathbb{E}} \left[ f(x) \right]$}{Expected value of the function $f$ given that samples $x$ are distributed according to $\mathcal{D}$}
\nomenclature{$G_t$}{Return from time step $t$}
\printnomenclature[0.9in]



%\begin{description}
%\item \textbf{Pr}(x) \tab probability of event x 
%\item s \tab state
%\item a  \tab  action
%\item r   \tab  reward
%\item $\mathcal{S}$    \tab set of all states
%\item $\mathcal{A}$     \tab set of all actions
%\item $\mathcal{R}$     \tab reward function
%\item $\Delta (\mathcal{X}) $   \tab set of  probability distributions over space $ \mathcal{X} $
%\item t \tab discrete time step
%\item$s_t$    \tab state at time t
%\item$a_t$   \tab  action at time t
%\item$\Theta$    \tab family of policy parameters $\Theta = \mathbb{R} ^d$
%\item$\boldsymbol{\theta}$    \tab policy parameter vector $\boldsymbol{\theta} \in \mathbb{R} ^d$
%\item$\Omega$   \tab  family of model parameters $\Omega = \mathbb{R} ^{d'}$ 
%\item$\boldsymbol{\omega}$    \tab model parameter vector $\boldsymbol{\omega} \in \mathbb{R} ^{d'}$
%\item$\pi$    \tab policy
%\item $\mathcal{P}$    \tab model 
%\item$\pi_\theta$    \tab policy parametrized by $\theta$
%\item $\mathcal{P}_\omega$    \tab  model parametrized by $\omega$
%\end{description}
