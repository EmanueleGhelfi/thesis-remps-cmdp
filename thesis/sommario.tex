\newpage
\chapter*{Abstract}

\addcontentsline{toc}{chapter}{Abstract}

The general goal of Reinforcement Learning (RL) is to design agents able to learn a behaviour from interactions with an environment.
Most of the problems tackled by Reinforcement Learning are typically modeled as Markov Decision Processes in which the environment is considered a fixed entity and cannot be controlled. Nevertheless, there exist several real-world examples in which a partial control on the environment can be exercised by the agent itself or by an external supervisor. For instance, in a car race the driver can set up his/her vehicle to better suit his/her needs. With the phrase environment configuration we refer to the activity of altering some environmental parameters to improve the performance of the agent's policy. This scenario has been recently formalized as a Configurable Markov Decision Process (CMDP). \newline
 The aim of this thesis is to further investigate the framework of Configurable Markov Decision Processes. We propose a new information theoretic algorithm, namely Relative Entropy Model Policy Search (REMPS), able to manage CMDPs with continuous action and state spaces. \newline
We propose a theoretical analysis of REMPS deriving the performance gap between the ideal case of the algorithm and the approximated case. Moreover, we empirically evaluate the performance of our approach in three scenarios, showing that it outperforms a naïve gradient method in several situations.

\chapter*{Estratto in Lingua Italiana}
\addcontentsline{toc}{chapter}{Estratto in Lingua Italiana}
L'\textit{Apprendimento per Rinforzo} \citep{sutton_introduction} è un campo dell'\textit{Intelligenza Artificiale} che tratta il problema dell'apprendimento tramite interazione con l'ambiente.
Questa disciplina considera \textit{problemi di decisioni sequenziali}, modellizzati come \textit{Processi Decisionali di Markov} \citep{Puterman:1994:MDP:528623}.
Il decisore, indicato come \textit{agente}, deve stabilire quale azione effettuare considerando l'incertezza dell'ambiente e la sua esperienza. 
Definiamo, in questo contesto, l'apprendimento come processo di adattamento dinamico del comportamento di un agente allo scopo di raggiungere un obiettivo.
Il comportamento dell'agente, che descrive quali azioni prendere in certe situazioni (stati), è chiamato \textit{politica}. Lo scopo degli algoritmi di Apprendimento per Rinforzo è quello di sviluppare agenti in grado di apprendere tramite interazione con l'ambiente, focalizzandosi sull'apprendimento di una politica che massimizzi una metrica di prestazione.
\paragraph{} Nella maggior parte dei problemi affrontati in letteratura, l'ambiente è considerato un'entità fissa, che non può essere controllata. Nonostante questo, esistono diversi esempi reali nei quali può essere esercitato un controllo parziale dell'ambiante dall'agente stesso o da un supervisore esterno. L'ambiente può essere quindi \textit{configurato} per massimizzare la velocità di apprendimento o la prestazione finale dell'agente.
Questo scenario è stato recentemente formalizzato come \textit{Processo Decisionale di Markov Configurabile} \citep{cmdp}. Risolvere un Processo Decisionale di Markov Configurabile significa trovare la politica dell'agente e la configurazione dell'ambiente che, congiuntamente, massimizzino le prestazioni. In \citep{cmdp} gli autori presentano un algoritmo di apprendimento sicuro, \textit{Safe Policy Model Iteration} (SPMI), per risolvere questo tipo di problema. Questo approccio è riuscito a mostrare i vantaggi della configurazione in esempi illustrativi, anche se è ben lungi dall'essere applicabile in scenari reali. Anzitutto, SPMI è applicabile solo a problemi con spazi di stati e azioni finiti, mentre molti esempi interessanti di Processi Decisionali di Markov Configurabili hanno almeno uno spazio di stati continuo. Secondariamente, questo algoritmo richiede una conoscenza esatta della dinamica dell'ambiente. Questa limitazione è rilevante, in quanto in realtà non conosciamo quasi mai la dinamica reale dell'ambiente e, anche se un modello può essere disponibile, questo può essere troppo approssimato o complesso per essere utilizzato (ad esempio il modello fluido-dinamico di una macchina). \newline
\paragraph{} In questa tesi vogliamo investigare più approfonditamente la relazione tra la politica e la configurazione dell'ambiente e trovare tecniche per ottimizzare questi componenti in modo congiunto. Presentiamo un algoritmo, derivato da \textit{Relative Entropy Policy Search} \citep{reps} (REPS), chiamato \textit{Relative Entropy Model Policy Search} (REMPS) che utilizza una formulazione primale-duale per aggiornare i parametri di politica e modello verso un ottimo locale. In aggiunta presentiamo una strategia di proiezione che tiene in considerazione l'effetto congiunto di politica e funzione di transizione. Studiamo le proprietà teoriche del nostro approccio analizzando l'errore dovuto alla differenza di prestazioni dal caso approssimato rispetto al caso ideale. 
La valutazione sperimentale di REMPS è effettuata su tre domini. Il primo dominio, chiamato catena, è semplice e viene utilizzato per visualizzare il comportamento del nostro algoritmo. Il secondo dominio, Cart-Pole, è un banco di prova classico per gli algoritmi di RL. Il terzo dominio, TORCS, è più complesso e mostra le prestazioni del nostro approccio in un problema di guida e configurazione automatica. Nei risultati sperimentali mostriamo che il nostro approccio è migliore rispetto a metodi a gradiente in alcune situazioni.\newline 
\paragraph{} La tesi è strutturata come segue. Il Capitolo \ref{Introduction} presenta il contesto, le motivazioni e lo scopo della tesi. Il Capitolo \ref{chapter2} è un'introduzione all'apprendimento per rinforzo e ai Processi Decisionali di Markov, insieme ad alcune importanti estensioni. Nel Capitolo \ref{chapter3} presentiamo lo Stato dell'Arte dei Processi Decisionali di Markov Configurabili e Relative Entropy Policy Search, siccome il nostro approccio si fonda su queste basi. Nel Capitolo \ref{chapter4} sviluppiamo i contributi principale di questa tesi: l'algoritmo Relative Entropy Model Policy Search e la sua analisi teorica. Nel Capitolo \ref{experimental_evaluation} mostriamo la valutazione sperimentale dell'algoritmo su tre esempi illustrativi. Il Capitolo \ref{capitolo7} contiene la conclusione, nella quale descriviamo brevemente il lavoro svolto e proponiamo possibili estensioni e direzioni di ricerca.